{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project Newton-GraphMamba Training\n",
        "\n",
        "This notebook trains the Graph-Mamba model on Kaggle's free GPU tier.\n",
        "\n",
        "## Setup Instructions\n",
        "1. Create a new Kaggle Notebook\n",
        "2. Enable GPU (Settings → Accelerator → GPU T4 x2)\n",
        "3. Run all cells sequentially\n",
        "4. Download the trained model from Output section\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Magic Install Block (Run FIRST!)\n",
        "\n",
        "This forces Mamba to install correctly on T4 GPUs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell FIRST in Kaggle\n",
        "# This installs PyTorch with CUDA 11.8 support\n",
        "!pip install torch==2.1.2+cu118 torchvision==0.16.2+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install ninja packaging\n",
        "!pip install causal-conv1d>=1.1.0\n",
        "!pip install mamba-ssm --no-build-isolation\n",
        "\n",
        "# Additional dependencies\n",
        "!pip install torch-geometric osmnx networkx pandas numpy tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "import osmnx as ox\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Import our model\n",
        "from newton_graphmamba import NewtonGraphMamba\n",
        "from graph_utils import networkx_to_pyg\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Download City Graph Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download city graph using OSMnx\n",
        "place_name = \"San Francisco, California, USA\"\n",
        "\n",
        "print(f\"Downloading graph for {place_name}...\")\n",
        "G = ox.graph_from_place(place_name, network_type='drive')\n",
        "\n",
        "print(f\"Graph nodes: {G.number_of_nodes()}\")\n",
        "print(f\"Graph edges: {G.number_of_edges()}\")\n",
        "\n",
        "# Convert to PyTorch Geometric format\n",
        "pyg_data = networkx_to_pyg(G)\n",
        "print(f\"PyG Data: {pyg_data}\")\n",
        "\n",
        "# Save graph data for later use\n",
        "torch.save(pyg_data, 'city_graph.pt')\n",
        "print(\"Graph saved to city_graph.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Initialize Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Model hyperparameters\n",
        "num_nodes = pyg_data.num_nodes\n",
        "node_features = pyg_data.x.shape[1] if pyg_data.x is not None else 6\n",
        "d_model = 256\n",
        "n_layers = 4\n",
        "d_state = 16\n",
        "d_conv = 4\n",
        "expand = 2\n",
        "\n",
        "# Initialize model\n",
        "model = NewtonGraphMamba(\n",
        "    num_nodes=num_nodes,\n",
        "    node_features=node_features,\n",
        "    d_model=d_model,\n",
        "    n_layers=n_layers,\n",
        "    d_state=d_state,\n",
        "    d_conv=d_conv,\n",
        "    expand=expand\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Prepare Training Data\n",
        "\n",
        "**Note:** Replace this with your actual trajectory data loader. This is a placeholder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_synthetic_routes(num_routes=1000, seq_len=100, num_nodes=None):\n",
        "    \"\"\"Generate synthetic route data for training.\n",
        "    \n",
        "    Replace this with your actual trajectory data loader.\n",
        "    \"\"\"\n",
        "    routes = []\n",
        "    vehicle_ids = []\n",
        "    \n",
        "    for _ in range(num_routes):\n",
        "        # Random walk on graph\n",
        "        route = torch.randint(0, num_nodes, (seq_len,))\n",
        "        routes.append(route)\n",
        "        vehicle_ids.append(torch.randint(0, 100, (1,)).item())\n",
        "    \n",
        "    return routes, vehicle_ids\n",
        "\n",
        "# Generate training data\n",
        "train_routes, train_vehicle_ids = generate_synthetic_routes(\n",
        "    num_routes=1000,\n",
        "    seq_len=100,\n",
        "    num_nodes=num_nodes\n",
        ")\n",
        "\n",
        "print(f\"Generated {len(train_routes)} training routes\")\n",
        "print(f\"Route length: {len(train_routes[0])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training hyperparameters\n",
        "batch_size = 32\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 10\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_losses = []\n",
        "    \n",
        "    # Batch training\n",
        "    for i in tqdm(range(0, len(train_routes), batch_size), desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        batch_routes = train_routes[i:i+batch_size]\n",
        "        batch_vehicle_ids = train_vehicle_ids[i:i+batch_size]\n",
        "        \n",
        "        # Pad sequences to same length\n",
        "        max_len = max(len(r) for r in batch_routes)\n",
        "        padded_routes = []\n",
        "        for route in batch_routes:\n",
        "            padded = F.pad(route, (0, max_len - len(route)), value=0)\n",
        "            padded_routes.append(padded)\n",
        "        \n",
        "        routes_tensor = torch.stack(padded_routes).to(device)\n",
        "        vehicle_ids_tensor = torch.tensor(batch_vehicle_ids, dtype=torch.long).to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Use routes as both graph_input and sequence_input for demo\n",
        "        # In real implementation, graph_input would be node features\n",
        "        logits = model.forward_decoder(\n",
        "            routes_tensor,\n",
        "            vehicle_id=vehicle_ids_tensor,\n",
        "            graph_memory=None  # Will be computed from graph\n",
        "        )\n",
        "        \n",
        "        # Create targets (next node prediction)\n",
        "        # Shift routes by 1 for next-node prediction\n",
        "        targets = routes_tensor[:, 1:].contiguous().view(-1)\n",
        "        logits_flat = logits[:, :-1].contiguous().view(-1, num_nodes)\n",
        "        \n",
        "        loss = criterion(logits_flat, targets)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_losses.append(loss.item())\n",
        "    \n",
        "    avg_loss = np.mean(epoch_losses)\n",
        "    losses.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model weights\n",
        "model_path = \"newton_mamba_v1.pt\"\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'model_config': {\n",
        "        'num_nodes': num_nodes,\n",
        "        'node_features': node_features,\n",
        "        'd_model': d_model,\n",
        "        'n_layers': n_layers,\n",
        "        'd_state': d_state,\n",
        "        'd_conv': d_conv,\n",
        "        'expand': expand\n",
        "    },\n",
        "    'training_losses': losses\n",
        "}, model_path)\n",
        "\n",
        "print(f\"Model saved to {model_path}\")\n",
        "print(f\"\\nDownload this file from the Output section of Kaggle!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
